{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQjq1AU49Q-q"
   },
   "source": [
    "# Домашнее задание №1: линейная регрессия (максимум 10 баллов)\n",
    "\n",
    "Некоторые задания будут по вариантам (всего 4 варианта). Чтобы выяснить свой вариант, посчитайте количество букв в своей фамилии, возьмете остаток от деления на 4 и прибавьте 1. (1 вариант у меня)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "1FsuljEb9Q-w"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChBks_AF9Q-y"
   },
   "source": [
    "## Многомерная линейная регрессия из sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jihDOyOd9Q-0"
   },
   "source": [
    "Создадим набор данных для многомерной регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "ykkouQff9Q-2",
    "outputId": "8223f891-c775-432e-c125-53bff94c60c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples = 10000)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMZP4pwT9Q-3"
   },
   "source": [
    "У нас 10000 объектов и 100 признаков. Для начала решим задачу аналитически \"из коробки\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "a-cl2ujU9Q-4",
    "outputId": "dc936815-cd0a-4c42-bc79-824337410051"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.637260890980729e-25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IKaW67f9Q-6"
   },
   "source": [
    "Теперь попробуем обучить линейную регрессию методом градиентного спуска \"из коробки\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Fzw6etrW9Q-7",
    "outputId": "9a1effb5-b3d9-47cc-bd8b-c696505c8521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4376973793436887e-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-7.40117592e-09,  7.14768035e+01,  1.62511378e-08, -5.62342799e-08,\n",
       "       -2.29130650e-09,  3.39077435e-08,  4.18334580e+01,  1.79319250e-08,\n",
       "        5.14434348e-09, -7.08661483e-09, -1.90969405e-08, -1.26304870e-08,\n",
       "       -1.00619438e-09, -4.28319020e-08, -1.97705430e-08, -3.02065913e-08,\n",
       "        2.70624736e+01,  4.72317206e-08, -4.05058294e-08,  1.11983847e+01,\n",
       "       -4.14530581e-10, -1.94581872e-08, -1.51324123e-08,  7.91050637e-09,\n",
       "       -7.10788408e-09, -1.26843500e-08, -1.73395388e-08, -1.26960054e-08,\n",
       "       -4.00751865e-08,  1.63275494e+01, -5.73345490e-10,  2.72234409e-08,\n",
       "       -3.98139723e-08,  1.77472378e-08,  6.41833147e-09, -6.28273121e-08,\n",
       "        1.29095871e-08,  1.20595804e-08, -3.26734523e-09, -1.01178498e-08,\n",
       "        6.11944223e+01, -1.43091637e-09, -2.61520464e-09,  2.27417925e-08,\n",
       "        4.31882641e-08, -6.80324841e-08, -2.01062448e-08, -1.68857064e-08,\n",
       "       -3.61060467e-08, -2.04470822e-08,  2.58472351e-08,  1.14209338e-08,\n",
       "       -7.50637981e-09, -1.28535294e-08,  2.35001927e+01, -1.69725010e-08,\n",
       "        1.34910806e-08, -2.65915228e-08,  1.80485319e-08,  9.45743641e-09,\n",
       "        1.09122177e-08,  1.16112350e-08, -2.53044691e-08,  1.47121056e-08,\n",
       "       -4.81476905e-08,  9.30986608e-09, -2.91472012e-08, -1.07221956e-08,\n",
       "       -4.17166880e-08,  1.39405962e-08,  3.32280222e-08, -1.86942833e-08,\n",
       "       -5.75463575e-08, -3.32364912e-08,  2.78748299e-08, -1.99125729e-08,\n",
       "       -5.54278733e-08, -3.85639237e-08,  4.60377259e-08,  2.02652133e-08,\n",
       "        4.08029707e+01,  4.27805862e-08, -6.02279013e-09, -4.56889236e-09,\n",
       "        2.34767882e-08,  1.20425502e+01,  9.93098749e+01, -1.28016975e-08,\n",
       "        3.60766615e-08,  1.45602013e-08, -1.28001353e-09, -2.41990205e-08,\n",
       "        2.04676160e-09,  3.01078004e-08, -5.64433102e-08,  1.42608783e-08,\n",
       "       -1.80773345e-08,  9.91806417e-09,  2.51404501e-08, -3.64709420e-08])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "reg = SGDRegressor(alpha=0.00000001).fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oq1076z-9Q-8"
   },
   "source": [
    "***Задание 1 (1 балл).*** Объясните, чем вызвана разница в значениях двух полученных значений метрики?\n",
    "\n",
    "***Задание 2 (1 балл).*** Подберите гиперпараметры в методе градиентного спуска так, чтобы значение MSE было близко к значению MSE, полученному при обучении LinearRegression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Задание 1) Разница в значениях MSE между LinearRegression и SGDRegressor вызвана тем, что:\n",
    "\n",
    "LinearRegression даёт точное аналитическое решение, минимизирующее MSE на данных без шума.\n",
    "SGDRegressor же из-за своей итеративной природы, возможной маленькой сходимости, наличия регуляризации и настройки параметров, не достигает такого же уровня минимизации MSE\n",
    "\n",
    "Задание 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005715668182927271\n"
     ]
    }
   ],
   "source": [
    "class LinearRegressionGD:\n",
    "    def __init__(self, alpha=0.01, max_iter=10000, tol=1e-6):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        self.mse_history = []\n",
    "        for i in range(self.max_iter):\n",
    "            y_pred = X.dot(self.theta)\n",
    "            error = y_pred - y\n",
    "            mse = np.mean(error ** 2)\n",
    "            self.mse_history.append(mse)\n",
    "            gradient = X.T.dot(error) / len(y)\n",
    "            self.theta -= self.alpha * gradient\n",
    "            if i > 0 and abs(self.mse_history[i] - self.mse_history[i-1]) < self.tol:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X.dot(self.theta)\n",
    "\n",
    "reg_gd = LinearRegressionGD(alpha=0.001, max_iter=10000, tol=1e-6)\n",
    "reg_gd.fit(X, y)\n",
    "print(np.mean((y - reg_gd.predict(X)) ** 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jJplHqS9Q-9"
   },
   "source": [
    "## Ваша многомерная линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKeXXhEH9Q--"
   },
   "source": [
    "***Задание 3 (5 баллов)***. Напишите собственную многомерную линейную регрессию, оптимизирующую MSE методом *градиентного спуска*. Для этого используйте шаблонный класс.\n",
    "\n",
    "Критерий останова: либо норма разности весов на текущей и предыдущей итерациях меньше определенного значения (первый и третий варианты), либо модуль разности функционалов качества (MSE) на текущей и предыдущей итерациях меньше определенного значения (второй и четвертый варианты). Также предлагается завершать обучение в любом случае, если было произведено слишком много итераций.\n",
    "\n",
    "***Задание 4 (2 балла)***. Добавьте l1 (первый и четвертый варианты) или l2 (второй и третий варианты) регуляризацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "VY3CTs0W9Q-_"
   },
   "outputs": [],
   "source": [
    "class LinearRegression(object):\n",
    "    def __init__(self, alpha=0.01, l_ratio=0.0, tol=1e-6, max_iter=10000):\n",
    "        '''\n",
    "        Инициализация параметров модели:\n",
    "        alpha - скорость обучения (learning rate)\n",
    "        l_ratio - коэффициент L1-регуляризации\n",
    "        tol - порог для критерия остановки по норме разности весов\n",
    "        max_iter - максимальное количество итераций обучения\n",
    "        '''\n",
    "        self.alpha = alpha                  \n",
    "        self.l_ratio = l_ratio            \n",
    "        self.tol = tol           \n",
    "        self.max_iter = max_iter           \n",
    "        self.weights = None             \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Метод для обучения линейной регрессии:\n",
    "        X - матрица признаков размерности (n_samples, n_features)\n",
    "        y - вектор целевых значений размерности (n_samples,)\n",
    "        '''\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        X = np.hstack([np.ones((n_samples, 1)), X])\n",
    "\n",
    "        self.weights = np.zeros(n_features + 1)          \n",
    "        weights_prev = np.full(n_features + 1, np.inf)   \n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            y_pred = np.dot(X, self.weights)\n",
    "\n",
    "            error = y_pred - y\n",
    "            gradient = (1 / n_samples) * np.dot(X.T, error) + self.l_ratio * np.sign(self.weights)\n",
    "\n",
    "            self.weights -= self.alpha * gradient\n",
    "\n",
    "            norm_diff = np.linalg.norm(self.weights - weights_prev)\n",
    "            if norm_diff < self.tol:\n",
    "                print(f'Сошлось за {iteration+1} итераций.')\n",
    "                break\n",
    "\n",
    "            weights_prev = self.weights.copy()\n",
    "        else:\n",
    "            print(f'Достигнуто максимальное количество итераций: {self.max_iter}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Метод для предсказания значений на новых данных:\n",
    "        X - матрица признаков размерности (n_samples, n_features)\n",
    "        '''\n",
    "        n_samples = X.shape[0]\n",
    "        X = np.hstack([np.ones((n_samples, 1)), X])\n",
    "        return np.dot(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "vH6XOYJm9Q-_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сошлось за 1576 итераций.\n",
      "You are amazing! Great work!\n"
     ]
    }
   ],
   "source": [
    "my_reg = LinearRegression()\n",
    "my_reg.fit(X, y)\n",
    "assert mean_squared_error(y, my_reg.predict(X)) < 1e-3\n",
    "print('You are amazing! Great work!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBRR_3Sh9Q_A"
   },
   "source": [
    "***Задание 5 (1 балл)***. Обучите линейную регрессию из коробки с l1-регуляризацией (from sklearn.linear_model import Lasso, первый и четвертый варианты) или с l2-регуляризацией (from sklearn.linear_model import Ridge, второй и третий варианты) с значением параметра регуляризации 0.1. Обучите вашу линейную регрессию с тем же значением параметра регуляризации и сравните результаты. Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "LrSPmUZl9Q_B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Достигнуто максимальное количество итераций: 10000\n",
      "Веса собственной линейной регрессии:\n",
      "[ 3.78086714e+00  1.40987530e+00 -1.87224272e+00  4.69158115e-04\n",
      "  2.88740319e+00  1.21507247e-05]\n",
      "\n",
      "Веса линейной регрессии из sklearn (Lasso):\n",
      "[ 3.88303711  1.42018829 -1.86972276  0.00659282  2.89537302 -0.        ]\n",
      "\n",
      "MSE собственной линейной регрессии: 0.3005145337930805\n",
      "MSE линейной регрессии из sklearn (Lasso): 0.2859302429787127\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "np.random.seed(0)\n",
    "n_samples = 100\n",
    "n_features = 5\n",
    "\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "true_weights = np.array([1.5, -2.0, 0.0, 3.0, 0.0])\n",
    "y = X @ true_weights + 4 + np.random.randn(n_samples) * 0.5 \n",
    "\n",
    "my_reg = LinearRegression(alpha=0.01, l_ratio=0.1, tol=1e-6, max_iter=10000)\n",
    "my_reg.fit(X, y)\n",
    "\n",
    "lasso_reg = Lasso(alpha=0.1, max_iter=10000, tol=1e-6)\n",
    "lasso_reg.fit(X, y)\n",
    "\n",
    "my_preds = my_reg.predict(X)\n",
    "lasso_preds = lasso_reg.predict(X)\n",
    "my_mse = mean_squared_error(y, my_preds)\n",
    "lasso_mse = mean_squared_error(y, lasso_preds)\n",
    "\n",
    "print(\"Веса собственной линейной регрессии:\")\n",
    "print(my_reg.weights)\n",
    "\n",
    "print(\"\\nВеса линейной регрессии из sklearn (Lasso):\")\n",
    "print(np.hstack([lasso_reg.intercept_, lasso_reg.coef_]))\n",
    "\n",
    "print(f\"\\nMSE собственной линейной регрессии: {my_mse}\")\n",
    "print(f\"MSE линейной регрессии из sklearn (Lasso): {lasso_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
